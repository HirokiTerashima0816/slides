### Python 機械学習プログラミング5.2

2018/02/01 @HirokiTerashima

---

### 項目

第5章 次元削減でデータを圧縮する
- 5.2 線形判別分析による教師データ圧縮

---

### 線形判別分析 (LDA)

- LDA はクラスの分離を最適化する特徴部分空間を見つける
- PCA は教師なしに対して LDA は教師あり
- 分類の特徴抽出としては LDA のほうが優れてる
  - もちろん PCA のほうが優れている場合もある
- 一般的に 「Fisher の線形判別」のことを指す
  - LDA でググるより Fisher 線形判別でググったほうが情報は出る

---

### LDA の前提

- 特徴量が正規分布に従う
- 特徴量が互いに独立
- クラスの共分散行列が同じ

(ただし多少満たしていなくてもそれなりにうまくいく)

---

### LDA の手順

1. d次元(dは特徴量の個数)のデータセットを標準化
2. クラスごとにd次元の平均ベクトルを出す
3. クラス間変動行列 $$S_B$$ とクラス内変動行列 $$S_W$$ を出す
4. 行列 $S_W^-1 S_B$ の固有ベクトルと対応する固有値を計算
5. $$d \times k$$ 次元の変換行列 $$W$$ を生成するために、最も大きいk個の固有値に対するk個の固有ベクトルを選択する
6. 変換行列 $W$ を使ってサンプルを新しい特徴部分空間へ射影する

---

### 算出について

- 基本計算が $$\sum$$ なので頑張ると `for` ラッシュになる
- 次元数が大きい場合は `numpy` とか使ってうまくやらないと遅くなる
- `sklearn.discriminant_analysis` の `LinearDiscriminantAnalysis` を使うと楽

---

### 参考

- [フィッシャーの線形判別分析法](https://qiita.com/pira/items/4c84399671be2cb598e4)
- [フィッシャーの線形判別](http://aidiary.hatenablog.com/entry/20100425/1272158587)
- [データマイニングの基礎:フィッシャーの線形判別とは](https://ameblo.jp/cyberanalyst/entry-11875422734.html)

上のサイトのグラフを見ると考え方はわかりやすい
