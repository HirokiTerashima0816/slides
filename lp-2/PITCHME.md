### 言語処理のための機械学習入門2

2018/04/05 @HirokiTerashima

---

### 項目

2 文書および単語の数学的表現
- 2.1 タイプ、トークン
- 2.2 nグラム
- 2.3 文書、文のベクトル表現
- 2.4 文書に対する前処理とデータスパースネス問題
- 2.5 単語のベクトル表現
- 2.6 文書や単語の確率分布による表現

---

### 2.1 タイプ、トークン

---

#### トークン(単語トークン)
単語一つ一つの出現を指す。その数を延べ語ともいう。

#### タイプ(単語タイプ)
単語の種類。異なり語数ともいう。

---

### 2.2 nグラム

---

#### 2.2.1 単語nグラム

- 隣り合って出現したn単語のこと。単語に関してなので**単語nグラム**という。
- n=1 => **ユニグラム**
- n=2 => **バイグラム**
- n=3 => **トリグラム**
- n≧4 => **nグラム**
- 単語列の最初と最後に**ダミー単語**を加えて、最初と最後に出現した単語の情報含めることができる。

---

#### 2.2.2 文字nグラム

隣り合って出現したn文字のこと。

---

### 2.3 文書、文のベクトル表現

---

#### 用語

- **素性**：ベクトルの要素が表す文書の特徴
- **素性値**：素性の値
- **特徴**：機械学習における素性
- **特徴量**：機械学習における素性値

---

#### 2.3.1 文書のベクトル表現、2.3.2 文のベクトル表現

- 各単語が何回出現したかをベクトル表現したものを**bag-of-words**。
- 文の構成や語順の情報は失われている。
- 単語の出現頻度で表しているので**頻度ベクトル**とよばれる。
- 出現したかどうかの true or false でベクトル化したものを**二値ベクトル**とよぶ。
- nグラムも含めて考える場合**bag-of-ngrams**ということもある。

---

### 2.4 文書に対する前処理とデータスパースネス問題

---

#### 2.4.1 文章に対しての前処理

- 話題の種類と関連を持たないと考えられる単語を**ストップワード**という。
- ストップワードを削除してからベクトル化することも多い。
- 派生語を含めて同一の素性とみなす作業を**ステミング**という。
- 英語のステミングでは**ポーターのステマー**がある。
  - 語尾の `ed` `ate` `ational` を除去するなど
- 単語を基本形に戻す作業を**見出し語化**とよぶ。
- 銀行の"bank"と土手の"bank"のように同じ単語の意味まで区別することを**語義の曖昧性解消**という。

---

#### 2.4.2 日本語の前処理

- 日本語は**形態素解析**をする。
- ステミングは用いられない。

---

#### 2.4.3 データスパースネス問題

- 0でない値をとる要素数が小さい傾向にある時、そのデータは**疎**であるという
- データが疎のとき、処理するために必要な統計値が十分に獲得できないことがある。この問題を**データスパースネス問題**という。

---

### 2.5 単語のベクトル表現

---

#### 2.5.1 単語トークンの文脈ベクトル

- 直前直後の1単語トークンを用いて表したベクトルを**文脈ベクトル**という
- 対象単語の前後の数トークンを考慮している時、文書中の考慮している箇所を**文脈窓**、その大きさを**文脈窓幅**という

---

#### 2.5.2 単語タイプの文脈ベクトル表現

- 単語タイプを文脈ベクトルで表現したもの
- 複数の文脈窓内でどんな単語が何回出現したかを素性とする

---

### 2.6 文書や単語の確率分布による表現

---

- 与えられた文章 $d$ 内で各単語がどの程度出現しやすいかを表す確率分布を考えて、これを文書を表しているとみなす
